{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5927526",
   "metadata": {},
   "source": [
    "## Building Blocks of LLM Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbeb7edb",
   "metadata": {},
   "source": [
    "There dimensions/Building blocks of LLM Engineering \n",
    "\n",
    "    1. Models : Open & Closed sources, Multimodal, Architecture and Selecting\n",
    "    2. Tools : HuggingFace, Langchain, Gradio, Weight & Biases, Modal\n",
    "    3. Techniques : Techniques,APIs, Multi-shot prompting, RAG, Fine-tuning and Agentization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582f325d",
   "metadata": {},
   "source": [
    "#Closed-Source Models:\n",
    "\n",
    "    • GPT from OpenAi\n",
    "\n",
    "    • Claude from Anthropic\n",
    "\n",
    "    • Gemini from Google\n",
    "\n",
    "    • Grok from x.ai\n",
    "\n",
    "#Open-Source Model: \n",
    " \n",
    "    • Llama from Meta\n",
    " \n",
    "    • Mixtral from Mistral\n",
    " \n",
    "    • Qwen from Alibaba Cloud\n",
    " \n",
    "    • Gemma from Google\n",
    " \n",
    "    • Phi from Miscroft\n",
    " \n",
    "    • Deepseek from Deepseek AI\n",
    " \n",
    "    • GPT-OSS from OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e87d1c",
   "metadata": {},
   "source": [
    "# Three ways to use models : \n",
    "\n",
    "1. Chat Interface(Like ChatGPT)\n",
    "\n",
    "2. Cloud APIs LLM APIs, Framesworks like LangChain. \n",
    "        Managed AI cloud services : Amazon Bedrock, Google Vertex, Azure ML\n",
    "\n",
    "3. Direct Inference, With the HuggingFace Transformer library With Ollama to run locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ffa6fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load and setup environment variables\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(override=True)\n",
    "api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "if not api_key:\n",
    "    print(\"No API key was found - please head over to the troubleshooting notebook in this folder to identify & fix!\")\n",
    "elif not api_key.startswith(\"sk-proj-\"):\n",
    "    print(\"An API key was found, but it doesn't start sk-proj-; please check you're using the right key - see troubleshooting notebook\")\n",
    "else:\n",
    "    print(\"API key found and looks good so far!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e3621c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a payload for an Endpoint request\n",
    "import requests\n",
    "\n",
    "headers = {\"Authorization\": f\"Bearer {api_key}\", \"Content-Type\": \"application/json\"}\n",
    "\n",
    "payload = {\n",
    "    \"model\": \"gpt-5-nano\",\n",
    "    \"messages\": [\n",
    "        {\"role\": \"user\", \"content\": \"Tell me a joke about programming.\"}\n",
    "    ]\n",
    "}\n",
    "\n",
    "payload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f6b9b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sending a request to the Endpoint\n",
    "response = requests.post(\n",
    "    \"https://api.openai.com/v1/chat/completions\",\n",
    "    headers=headers,\n",
    "    json=payload\n",
    ")\n",
    "\n",
    "response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7409b0bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "response.json()['choices'][0]['message']['content']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e29e82",
   "metadata": {},
   "source": [
    "# OpenAI Package\n",
    "\n",
    "• Python Client Library\n",
    "\n",
    "• wrapper around making this exact call to http endpoint\n",
    "\n",
    "• Allow you to work with nice Python code instead of messing around with janky json object\n",
    "\n",
    "• Open-source and light weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b2a55b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Endpoint call over http call\n",
    "from openai import OpenAI\n",
    "openai = OpenAI()\n",
    "\n",
    "response = openai.chat.completions.create(\n",
    "    model=\"gpt-5-nano\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Tell me a joke about programming.\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc44822a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Working with gemini\n",
    "GEMINI_BASE_URL = \"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "google_api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "\n",
    "if not google_api_key:\n",
    "    print(\"No API key was found - please be sure to add your key to the .env file, and save the file! Or you can skip the next 2 cells if you don't want to use Gemini\")\n",
    "elif not google_api_key.startswith(\"AIz\"):\n",
    "    print(\"An API key was found, but it doesn't start AIz\")\n",
    "else:\n",
    "    print(\"API key found and looks good so far!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd47a4a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "gemini = OpenAI(base_url=GEMINI_BASE_URL, api_key=google_api_key)\n",
    "\n",
    "response = gemini.chat.completions.create(\n",
    "    model=\"gemini-2.5-pro\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Tell me a joke about programming.\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9fa259d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Working with Ollama\n",
    "requests.get(\"http://localhost:11434\").content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f612992",
   "metadata": {},
   "source": [
    "## Download llama3.2:1b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a2e87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ollama pull llama3.2:1b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2d984d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "OLLAMA_BASE_URL = \"http://localhost:11434/v1\"\n",
    "\n",
    "ollama = OpenAI(base_url=OLLAMA_BASE_URL, api_key='ollama')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429e4e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = ollama.chat.completions.create(model=\"llama3.2:1b\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Tell me a joke about programming.\"}]\n",
    ")\n",
    "response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e139a55a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ollama pull deepseek-r1:1.5b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16650125",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Working with smaller deepseek model\n",
    "response = ollama.chat.completions.create(model=\"deepseek-r1:1.5b\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Tell me a joke about programming.\"}]\n",
    ")\n",
    "response.choices[0].message.content"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
